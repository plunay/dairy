---
title: '20190527'
created: '2019-05-27T02:32:37.633Z'
modified: '2019-06-12T06:11:59.783Z'
attachments: [attention_mechanism.jpg, expression_of_Probabilistic Language Model.png]
tags: [attention]
---

# 20190527

## Attention

### Core of probabilistic language model

Asign a probability to a sentence by Markov Assumption

![Icon](@attachment/expression_of_Probabilistic Language Model.png)

### Vanilla RNN (the classic one)'s problem

* Can only handle fixed-length problem.
* Suffer from Gradient Vanishing/Exploding, hard to train when sentences are long.

To slove these, encoder-decoder model is adopted and RNN cell is changed to GRU or LSTM, tanh to Relu.

![Icon](@attachment/encoder_decoder_model.png)

Is one hidden state really enough?

![Icon](@attachment/attention_mechanism.jpg)

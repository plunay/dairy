---
title: '20190527'
created: '2019-05-27T02:32:37.633Z'
modified: '2019-06-12T08:39:50.737Z'
attachments: [attention_mechanism.jpg, encoder_decoder_model.png, expression_of_Probabilistic_Language_Model.png]
tags: [attention]
---

# 20190527

## Attention

### Core of probabilistic language model

Asign a probability to a sentence by Markov Assumption

![Icon](../attachments/expression_of_Probabilistic_Language_Model.png)

### Vanilla RNN (the classic one)'s problem

* Can only handle fixed-length problem.
* Suffer from Gradient Vanishing/Exploding, hard to train when sentences are long.

To slove these, encoder-decoder model is adopted and RNN cell is changed to GRU or LSTM, tanh to Relu.

![Icon](../attachments/encoder_decoder_model.png)

Is one hidden state really enough?

![Icon](../attachments/attention_mechanism.jpg)
